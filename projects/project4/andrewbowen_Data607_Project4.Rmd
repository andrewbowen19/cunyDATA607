---
title: "DATA 607 Project 4"
author: "Andrew Bowen"
date: "2022-11-15"
output: html_document
---

```{r setup, message=FALSE}
library(stringr)
library(tensorflow)
library(tfdatasets)
library(keras)
library(glue)
library(ff)

# Call this if you need tensorflow installed
# install_tensorflow()
```


I'd like to try some text classification using the `tensorflow` package in R. I'm leaning on this [tutorial page](https://tensorflow.rstudio.com/tutorials/keras/text_classification) to get some of the basics, as I've only used tensorflow in python before, not R

Loading in our [ham dataset](https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2)
```{r}
# Loading ham dataset
ham_url <- "https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2"

ham_dataset <- get_file(
  "data/20021010_easy_ham",
  ham_url,
  untar = TRUE,
  cache_dir = '.',
  cache_subdir = ''
)

# Loading spam dataset
spam_url <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"

spam_dataset <- get_file(
  "data/20030228_spam_2",
  spam_url,
  untar = TRUE,
  cache_dir = '.',
  cache_subdir = ''
)
```


```{r}
ham_dir <- file.path("easy_ham")
spam_dir <- file.path("spam_2")

# Moving our dataset files toa  "Centralized" data folder
file.move("easy_ham/", "data/easy_ham/")
file.move("spam_2/", "data/spam_2/")
```

```{r}
# Renaming unzipped files in our spam and ham dirs so they have a ,txt extension
dirs <- c("spam_2", "easy_ham")

for (d in dirs) {
  dir_path <- glue("data/{d}/")
  for (f in list.files(dir_path)) {
    if (!grepl(".txt", f)) {
      new_name <- glue("{dir_path}{f}.txt")
      file.rename(glue("{dir_path}{f}"), new_name)
    }
  }
}

```

Generating training datasets from our directories containing the raw text files. [`keras` looks for the sub-folders](https://datascience.stackexchange.com/questions/51671/keras-flow-from-directory-returns-0-images) within our data folder as the sub-classes
```{r}
seed <- 1234


training_data <- text_dataset_from_directory(
  'data/',
  batch_size = 64,
  validation_split = 0.1,
  subset = 'training',
  seed = seed
)

# Validation data - 10% of dataset in total ( ~ 390 files)
validation_data <- text_dataset_from_directory(
  'data/',
  batch_size = 64,
  validation_split = 0.1,
  subset = 'validation',
  seed = seed
)
```


```{r}

re <- reticulate::import("re")

punctuation <- c("!", "\\", "\"", "#", "$", "%", "&", "'", "(", ")", "*",
"+", ",", "-", ".", "/", ":", ";", "<", "=", ">", "?", "@", "[",
"\\", "\\", "]", "^", "_", "`", "{", "|", "}", "~")
punctuation_group <- punctuation %>%
  sapply(re$escape) %>%
  paste0(collapse = "") %>%
  sprintf("[%s]", .)

# Writing our standardization function
standardization_function <- function(input_data) {
  lowercase <- tf$strings$lower(input_data)
  stripped_html <- tf$strings$regex_replace(lowercase, '<*>', ' ')
  tf$strings$regex_replace(
    stripped_html,
    punctuation_group,
    ""
  )
}

max_features <- 10000
sequence_length <- 250

# Cleaning and tokenizing data
vectorize_layer <- layer_text_vectorization(
  standardize = standardization_function,
  max_tokens = max_features,
  output_mode = "int",
  output_sequence_length = sequence_length
)

```




```{r}
# Make a text-only dataset (without labels), then call adapt
train_text <-training_data %>%
  dataset_map(function(text, label) text)
vectorize_layer %>% adapt(train_text)
```


```{r}

vectorize_text <- function(text, label) {
  text <- tf$expand_dims(text, -1L)
  list(vectorize_layer(text), label)
}

```


Setting up training, testing, and validation datasets
```{r}
train_ds <- dataset_map(training_data, vectorize_text)
val_ds <- validation_data %>% dataset_map(vectorize_text)
```


```{r}

model <- keras_model_sequential() %>%
  layer_embedding(max_features + 1, 16) %>%
  layer_dropout(0.2) %>%
  layer_global_average_pooling_1d() %>%
  layer_dropout(0.2) %>%
  layer_dense(1)

summary(model)
```

Need to [compile our model](https://rdrr.io/cran/kerasR/man/keras_compile.html) before training the model. This modifies the model object itself
```{r}

model %>% compile(
  loss = loss_binary_crossentropy(from_logits = TRUE), # using this loss function as it's a binary outcome datase (i.e. either ham or spam)
  optimizer = 'adam',
  metrics = metric_binary_accuracy(threshold = 0)) %>%
  fit(train_ds, validation_data = val_ds, epochs = 10)
```



### Testing our Model
Now that we have our model built and fit, let's use it to predict spam vs ham for some other emails. Using some samples from the [default spam dataset](https://spamassassin.apache.org/old/publiccorpus/20030228_spam.tar.bz2) to set up our testing data set. 
```{r}
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
  'data/test',
  batch_size = 64
)
```

Setting up and evaluating our testing dataset with our model
```{r}
testing_ds <- testing_data %>% dataset_map(vectorize_text)

# Evaluate the model
model %>% evaluate(testing_ds)
```




