?list.files
for (f in list.files("./data/test")){
print(f)
}
for (f in list.files("./data/test")){
file_text <- readLines(f)
print(file_text)
}
for (f in list.files("./data/test")){
file_text <- readLines(glue("./data/test{f}"))
print(file_text)
}
for (f in list.files("./data/test")){
file_path <- glue("./data/test{f}")
file_text <- readLines(file_path)
print(file_text)
}
for (f in list.files("./data/test")){
file_path <- glue("./data/test{f}")
# file_text <- readLines(file_path)
print(file_path)
}
for (f in list.files("./data/test")){
file_path <- glue("./data/test/{f}")
# file_text <- readLines(file_path)
print(file_path)
}
for (f in list.files("./data/test")){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
print(file_path)
}
for (f in list.files("./data/test")){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
print(file_text)
}
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
file_text_cleaned <- dataset_map(vectorize_text, file_text)
predict(testing_model, file_text_cleaned)
}
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
file_text_cleaned <- dataset_map(vectorize_text, file_text)
# predict(testing_model, file_text_cleaned)
}
?dataset_map
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
file_text_cleaned <- dataset_map(file_text, vectorize_text)
print(file_text_cleaned)
# predict(testing_model, file_text_cleaned)
}
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
file_text_cleaned <- dataset_map(file_text, standardization_function())
print(file_text_cleaned)
# predict(testing_model, file_text_cleaned)
}
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path)
file_text_cleaned <- dataset_map(file_text, standardization_function)
print(file_text_cleaned)
# predict(testing_model, file_text_cleaned)
}
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
# file_text_cleaned <- dataset_map(file_text, standardization_function)
# print(file_text_cleaned)
predict(testing_model, file_text)
}
sample_emails = c()
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
append(sample_emails, file_text)
}
predict(testing_model, sample_emails)
sample_emails = c()
for (f in list.files("./data/test")[0:10]){
file_path <- glue("./data/test/{f}")
print(file_path)
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
append(sample_emails, file_text)
}
predict(testing_model, sample_emails)
sample_emails = c()
file_list <- list.files("./data/test")[0:10]
for (i in seq_along(file_list)){
fname <- names(file_list)[i]
file_path <- glue("./data/test/{f}")
print(file_path)
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
append(sample_emails, file_text)
}
predict(testing_model, sample_emails)
sample_emails = c()
file_list <- list.files("./data/test")[0:10]
for (i in seq_along(file_list)){
fname <- names(file_list)[i]
file_path <- glue("./data/test/{f}")
print(file_path)
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
sample_emails[[i]] <- file_text
}
predict(testing_model, sample_emails)
sample_emails = c()
file_list <- list.files("./data/test")[0:10]
for (i in seq_along(file_list)){
fname <- names(file_list)[i]
file_path <- glue("./data/test/{f}")
print(file_path)
file_text <- readLines(file_path) %>%
str_replace_all("<*>", "") %>%
str_to_lower()
sample_emails[[i]] <- file_text
}
predict(testing_model, sample_emails)
View(sample_emails)
sample_emails[[1]]
testing_model %>% predict(c("Great deal! Nigerian prince"))
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/',
batch_size = 64,
seed = seed
)
# Cleaning and tokenizing testing data
testing_vectorize_layer <- layer_text_vectorization(
standardize = standardization_function,
max_tokens = max_features,
output_mode = "int",
output_sequence_length = sequence_length
)
testing_ds <- testing_data $>$ dataset_map(vectorize_text)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
predict(testing_model, testing_ds)
predict(model, testing_ds)
predict(model, testing_ds)
testing_ds <- testing_vectorize_layer %>% dataset_map(vectorize_text)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/',
batch_size = 64,
seed = seed
)
# Cleaning and tokenizing testing data
testing_vectorize_layer <- layer_text_vectorization(
standardize = standardization_function,
max_tokens = max_features,
output_mode = "int",
output_sequence_length = sequence_length
)
testing_ds <- testing_vectorize_layer %>% dataset_map(vectorize_text)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
predict(model, testing_ds)
predict(model, testing_ds)
predict(model, testing_ds)
predict(model, val_ds)
# predict(model, testing_ds)
model %>% evaluate(testing_ds)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/',
batch_size = 64
)
# Cleaning and tokenizing testing data
testing_vectorize_layer <- layer_text_vectorization(
standardize = standardization_function,
max_tokens = max_features,
output_mode = "int",
output_sequence_length = sequence_length
)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/',
batch_size = 64
)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
# predict(model, testing_ds)
model %>% evaluate(testing_ds)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/test/',
batch_size = 64
)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
# predict(model, testing_ds)
model %>% evaluate(testing_ds)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/test',
batch_size = 64
)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
# predict(model, testing_ds)
model %>% evaluate(testing_ds)
library(stringr)
library(tensorflow)
library(tfdatasets)
library(keras)
library(glue)
library(ff)
# Call this if you need tensorflow installed
# install_tensorflow()
# Loading ham dataset
ham_url <- "https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2"
ham_dataset <- get_file(
"data/20021010_easy_ham",
ham_url,
untar = TRUE,
cache_dir = '.',
cache_subdir = ''
)
# Loading spam dataset
spam_url <- "https://spamassassin.apache.org/old/publiccorpus/20030228_spam_2.tar.bz2"
spam_dataset <- get_file(
"data/20030228_spam_2",
spam_url,
untar = TRUE,
cache_dir = '.',
cache_subdir = ''
)
ham_dir <- file.path("easy_ham")
spam_dir <- file.path("spam_2")
# Moving our dataset files toa  "Centralized" data folder
file.move("easy_ham/", "data/easy_ham/")
file.move("spam_2/", "data/spam_2/")
# Renaming unzipped files in our spam and ham dirs so they have a ,txt extension
dirs <- c("spam_2", "easy_ham")
for (d in dirs) {
dir_path <- glue("data/{d}/")
for (f in list.files(dir_path)) {
if (!grepl(".txt", f)) {
new_name <- glue("{dir_path}{f}.txt")
file.rename(glue("{dir_path}{f}"), new_name)
}
}
}
seed <- 1234
training_data <- text_dataset_from_directory(
'data/',
batch_size = 64,
validation_split = 0.1,
subset = 'training',
seed = seed
)
# Validation data - 10% of dataset in total ( ~ 390 files)
validation_data <- text_dataset_from_directory(
'data/',
batch_size = 64,
validation_split = 0.1,
subset = 'validation',
seed = seed
)
re <- reticulate::import("re")
punctuation <- c("!", "\\", "\"", "#", "$", "%", "&", "'", "(", ")", "*",
"+", ",", "-", ".", "/", ":", ";", "<", "=", ">", "?", "@", "[",
"\\", "\\", "]", "^", "_", "`", "{", "|", "}", "~")
punctuation_group <- punctuation %>%
sapply(re$escape) %>%
paste0(collapse = "") %>%
sprintf("[%s]", .)
# Writing our standardization function
standardization_function <- function(input_data) {
lowercase <- tf$strings$lower(input_data)
stripped_html <- tf$strings$regex_replace(lowercase, '<*>', ' ')
tf$strings$regex_replace(
stripped_html,
punctuation_group,
""
)
}
max_features <- 10000
sequence_length <- 250
# Cleaning and tokenizing data
vectorize_layer <- layer_text_vectorization(
standardize = standardization_function,
max_tokens = max_features,
output_mode = "int",
output_sequence_length = sequence_length
)
# Make a text-only dataset (without labels), then call adapt
train_text <-training_data %>%
dataset_map(function(text, label) text)
vectorize_layer %>% adapt(train_text)
vectorize_text <- function(text, label) {
text <- tf$expand_dims(text, -1L)
list(vectorize_layer(text), label)
}
train_ds <- dataset_map(training_data, vectorize_text)
val_ds <- validation_data %>% dataset_map(vectorize_text)
model <- keras_model_sequential() %>%
layer_embedding(max_features + 1, 16) %>%
layer_dropout(0.2) %>%
layer_global_average_pooling_1d() %>%
layer_dropout(0.2) %>%
layer_dense(1)
summary(model)
model %>% compile(
loss = loss_binary_crossentropy(from_logits = TRUE), # using this loss function as it's a binary outcome datase (i.e. either ham or spam)
optimizer = 'adam',
metrics = metric_binary_accuracy(threshold = 0)) %>%
fit(train_ds, validation_data = val_ds, epochs = 10)
# Setting up our testing dataset
testing_data <- text_dataset_from_directory(
'data/test',
batch_size = 64
)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
# predict(model, testing_ds)
model %>% evaluate(testing_ds)
testing_ds <- testing_data %>% dataset_map(vectorize_text)
# Evaluate the model
model %>% evaluate(testing_ds)
model_sgd <- keras_model_sequential() %>%
layer_embedding(max_features + 1, 16) %>%
layer_dropout(0.2) %>%
layer_global_average_pooling_1d() %>%
layer_dropout(0.2) %>%
layer_dense(1)
summary(model_sgd)
# Compiling our model with an SGD optimizer
model_sgd %>% compile(
loss = loss_binary_crossentropy(from_logits = TRUE),
optimizer = 'SGD',
metrics = metric_binary_accuracy(threshold = 0)) %>%
fit(train_ds, validation_data = val_ds, epochs = 10)
model_sgd %>% evaluate(testing_dataset)
testing_ddataset <- testing_data %>% dataset_map(vectorize_text)
# Evaluate the model
model %>% evaluate(testing_dataset)
testing_dataset <- testing_data %>% dataset_map(vectorize_text)
# Evaluate the model
model %>% evaluate(testing_dataset)
model_sgd %>% evaluate(testing_dataset)
model_sgd <- keras_model_sequential() %>%
layer_embedding(max_features + 1, 16) %>%
layer_dropout(0.2) %>%
layer_global_average_pooling_1d() %>%
layer_dropout(0.2) %>%
layer_dense(1)
summary(model_sgd)
# Compiling our model with an SGD optimizer
model_sgd %>% compile(
loss = loss_binary_crossentropy(from_logits = TRUE),
optimizer = 'SGD',
metrics = metric_binary_accuracy(threshold = 0)) %>%
fit(train_ds, validation_data = val_ds, epochs = 20)
model_sgd %>% evaluate(testing_dataset)
knitr::opts_chunk$set(echo = FALSE)
library(httr)
library(jsonlite)
library(tidyverse)
library(sf) # for geospatial data lookup
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- VERB("GET", url, query = queryString,
content_type("application/octet-stream"),
accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- VERB("GET", url, query = queryString,
# content_type("application/octet-stream"),
accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- VERB("GET", url, query = queryString)
# content_type("application/octet-stream"),
# accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- VERB("GET", url, query = queryString
content_type("application/octet-stream"),
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- VERB("GET", url, query = queryString,
content_type("application/octet-stream"),
accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
View(response)
?GET
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- GET(url, query = queryString,
content_type("application/octet-stream"),
accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
reticulate::repl_python()
# load data
data_url <- "https://raw.githubusercontent.com/fivethirtyeight/data/master/fifa/fifa_countries_audience.csv"
fifa <- read.csv(data_url)
summary(fifa)
library(ggplot2)
library(dplyr)
library(maps)
library(stringr)
# load data
data_url <- "https://raw.githubusercontent.com/fivethirtyeight/data/master/fifa/fifa_countries_audience.csv"
fifa <- read.csv(data_url)
# Adding continent column for cleaner plots/readability
confederation <- c("CONCACAF", "UEFA", "CONMEBOL", "AFC", "CAF", "OFC")
continent <- c("North America", "Europe", "South America", "Asia", "Africa", "Oceania")
continent_map <- data.frame(confederation, continent)
fifa <- merge(fifa, continent_map)
fifa <- fifa %>%
mutate(european = ifelse(fifa$confederation == "UEFA", TRUE, FALSE))
summary(fifa)
# GDP weighted viewership share
ggplot(fifa, aes(x = gdp_weighted_share)) + geom_histogram()
# population share
ggplot(fifa, aes(x = population_share)) + geom_histogram()
# TV audience share
ggplot(fifa, aes(x = tv_audience_share)) + geom_histogram()
world_map <- map_data("world") %>%
rename("country" = "region")
# USA named differently in each dataset, replacing to standardize column values (same wqith UK)
world_map$country <- str_replace(world_map$country, "USA", "United States")
world_map$country <- str_replace(world_map$country, "UK", "United Kingdom")
# Joining our world_map geographic data to our World Cup viewership data by country
fifa_coords <- world_map %>%
left_join(fifa, by = c("country"))
head(fifa_coords)
ggplot(fifa_coords, aes(long, lat, group = group)) +
geom_polygon(aes( group=group, fill=confederation)) +
ggtitle("FIFA Confederation memberdship by country") +
xlab("Longitude (deg)") + ylab("Latitude (deg)")
# Plotting TV Audience share by country
ggplot(fifa_coords, aes(long, lat, group = group)) +
geom_polygon(aes( group=group, fill=tv_audience_share)) +
ggtitle("TV Audience share (%) of world cup viewership by country") +
xlab("Longitude (deg)") + ylab("Latitude (deg)")
ggplot(fifa_coords, aes(long, lat, group = group)) +
geom_polygon(aes( group=group, fill=gdp_weighted_share)) +
ggtitle("GDP-weighted share (%) of world cup viewership by country") +
xlab("Longitude (deg)") + ylab("Latitude (deg)")
# Filtering into our two groups: Europe vs not Europe.
europe <- fifa %>% filter(european== TRUE)
other_countries <- fifa %>% filter(european == FALSE)
# Running one-tailed t-test using R built-in
t.test(europe$gdp_weighted_share, other_countries$gdp_weighted_share, alternative="greater")
# Running one-tailed t-test using R built-in
t.test(europe$tv_audience_share, other_countries$tv_audience_share, alternative="greater")
confederation_test <- aov(tv_audience_share ~ confederation, data = fifa)
summary(confederation_test)
knitr::opts_chunk$set(echo = FALSE)
library(httr)
library(jsonlite)
library(tidyverse)
library(sf) # for geospatial data lookup
url <- "https://api.openaq.org/v2/measurements"
queryString <- list(
sort = "desc",
radius = "1000",
order_by = "datetime",
city = "New York"
)
response <- GET(url, query = queryString,
content_type("application/octet-stream"),
accept("application/json"))
as.data.frame(fromJSON(rawToChar(content(response, "raw"))))
reticulate::repl_python()
