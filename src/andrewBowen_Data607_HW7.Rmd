---
title: "Andrew Bowen DATA 607 HW 7"
author: "Andrew Bowen"
date: "2022-11-01"
output: html_document
bibliography: references.bib
---

Using the sample code from [Chapter 2 of Text Mining with R](https://www.tidytextmining.com/sentiment.html)
```{r setup, message=FALSE}
library(tidytext)
library(tidyr)
library(janeaustenr)
library(dplyr)
library(stringr)
library(ggplot2)
library(wordcloud)
library(reshape2)
library(glue)
```

```{r}
get_sentiments("afinn")
get_sentiments("bing")
get_sentiments("nrc")
```


```{r, message=FALSE}
tidy_books <- austen_books() %>%
  group_by(book) %>%
  mutate(
    linenumber = row_number(),
    chapter = cumsum(str_detect(text, 
                                regex("^chapter [\\divxlc]", 
                                      ignore_case = TRUE)))) %>%
  ungroup() %>%
  unnest_tokens(word, text)

```

```{r}
nrc_joy <- get_sentiments("nrc") %>% 
  filter(sentiment == "joy")

tidy_books %>%
  filter(book == "Emma") %>%
  inner_join(nrc_joy) %>%
  count(word, sort = TRUE)
```

```{r}
jane_austen_sentiment <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(book, index = linenumber %/% 80, sentiment) %>%
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>% 
  mutate(sentiment = positive - negative)

# Plotting the sentiment scores
ggplot(jane_austen_sentiment, aes(index, sentiment, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```
Comparing NRC lexibon to others
```{r}
get_sentiments("nrc") %>% 
  filter(sentiment %in% c("positive", "negative")) %>% 
  count(sentiment)

get_sentiments("bing") %>% 
  count(sentiment)

bing_word_counts <- tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  ungroup()

bing_word_counts %>%
  group_by(sentiment) %>%
  slice_max(n, n = 10) %>% 
  ungroup() %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Contribution to sentiment",
       y = NULL)
```


```{r}
custom_stop_words <- bind_rows(tibble(word = c("miss"),  
                                      lexicon = c("custom")), 
                               stop_words)
```

Generating word clouds
```{r}
tidy_books %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 100))

tidy_books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE) %>%
  acast(word ~ sentiment, value.var = "n", fill = 0) %>%
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```
Now we'll look at a sentence level rather than a word level
```{r}
p_and_p_sentences <- tibble(text = prideprejudice) %>% 
  unnest_tokens(sentence, text, token = "sentences")

# can also split tokens via regex
austen_chapters <- austen_books() %>%
  group_by(book) %>%
  unnest_tokens(chapter, text, token = "regex", 
                pattern = "Chapter|CHAPTER [\\dIVXLC]") %>%
  ungroup()

austen_chapters %>% 
  group_by(book) %>% 
  summarise(chapters = n())
```

```{r}
bingnegative <- get_sentiments("bing") %>% 
  filter(sentiment == "negative")

wordcounts <- tidy_books %>%
  group_by(book, chapter) %>%
  summarize(words = n())

tidy_books %>%
  semi_join(bingnegative) %>%
  group_by(book, chapter) %>%
  summarize(negativewords = n()) %>%
  left_join(wordcounts, by = c("book", "chapter")) %>%
  mutate(ratio = negativewords/words) %>%
  filter(chapter != 0) %>%
  slice_max(ratio, n = 1) %>% 
  ungroup()

```


## Part 1
Using a different corpus and sentiment lexicon, as well as corpus. For my corpus/dataset, I chose the [Harry Potter novels](https://uc-r.github.io/sentiment_analysis), included in the `harrypotter` library. I'll also be using the [Jockers lexicon](https://rdrr.io/cran/lexicon/man/hash_sentiment_jockers_rinker.html) included in the `lexicon` package, which includes polarity scores as lookup values for 11,710 words.

```{r}
library(harrypotter)
library(sentimentr)
library(lexicon)
```

Getting Jockers lexicon into a dataframe and renaming columns for readability
```{r}
j_lexicon <- as.data.frame(lexicon::hash_sentiment_jockers)
j_lexicon <- j_lexicon %>%
          rename("word" = "x", "score" = "y")
```

We'll be using the text of *Harry Potter and the Chamber of Secrets* in our example. We pulled the corpus from [this repo of .rda files](https://github.com/bradleyboehmke/harrypotter/blob/master/data/) included in the containing the text of each Harry Potter book (7 in total). Inorder to make this code reproducible, [I used the script `exxtract_hp_data.R`](https://github.com/andrewbowen19/cunyDATA607/blob/main/src/extract_hp_data.R) included in this repo to pull in the `.Rda` data and dump to csv files [hosted on my GitHub (broken out by chapter)](https://github.com/andrewbowen19/cunyDATA607/tree/main/data/harrypotter). 

Each csv in the linked repo corresponds to one chapter of a book from the Harry Potter series. For instance, chapter 5 of the Chamber of Secrets would be contained in the [`chamber_of_secrets5.csv` file](https://github.com/andrewbowen19/cunyDATA607/blob/main/data/harrypotter/chamber_of_secrets5.csv)

First, let's read in each csv file and dump them into a single dataframe, including book labels for plotting later.
```{r}
chapter_lengths <- c("philosophers_stone" = seq(1, 17),
                     "chamber_of_secrets" = seq(1, 19),
                     "prisoner_of_azkaban" = seq(1, 22),
                     "goblet_of_fire" = seq(1, 37),
                     "order_of_the_phoenix" = seq(1, 22),
                     "half_blood_prince" = seq(1, 30),
                     "deathly_hallows" = seq(1, 37)
                     )

# Looping over each of our chapter files within our GitHub repo
# appending each corresponding csv's dataframe to our overall one
datalist = vector("list", length = length(names(chapter_lengths)))
i <- 1
for (chapter in names(chapter_lengths)){
  url <- glue("https://raw.githubusercontent.com/andrewbowen19/cunyDATA607/main/data/harrypotter/{chapter}.csv", header=TRUE)
  book_name <- str_replace_all(chapter, "\\d{1,2}", "")
  chapter_df <- read.csv(url, sep=",",  header=TRUE) %>%
                mutate(book = book_name)

  # chapter_df$book <- seq
  
  datalist[[i]] <- chapter_df
  
  i <- i + 1

}

df <- do.call(rbind, datalist)
head(df)
```

```{r}
# text_cs <- tibble(chapter = seq_along(chamber_of_secrets),
#                   text = chamber_of_secrets) %>%
#                   unnest_tokens(word, text) %>%
#                   mutate(book = "Chamber of Secrets")
```

Doing an inner join to get our lexicon scores matched up with the words in the text
```{r}
# cs_scores <- text_cs %>%
#   inner_join(j_lexicon) %>%
#   count(word, score,  sort = TRUE) %>%
#   ungroup()
```



Plotting our results as done above in the Jane Austen example code - these sentiment scores per word are a bit busy. 
```{r}
df %>% 
      mutate(index = row_number()) %>%
      ggplot( aes(index, score)) +
        geom_col(show.legend = FALSE) 
```

Let's re-create the above plot with the scores colored by book.
```{r plot-score-all-books}
df %>% 
      mutate(index = row_number()) %>%
      ggplot( aes(index, score, fill=book)) +
        geom_col( )
```

Here is the same as above (sentiment scores per word per book) broken out into columns (as above)
```{r}
df_sentiment <- df %>%
  left_join(j_lexicon) %>%
  count(book, index = c(1:nrow(df)), score)

# Plotting the sentiment scores
ggplot(df_sentiment, aes(index, score, fill = book)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~book, ncol = 2, scales = "free_x")
```

I'd also 
```{r sentiment-scores-hist}
ggplot(df, aes(x=score)) + geom_histogram(binwidth=0.1)
```


```{r mean-score-by-book}
df %>%
  group_by(book) %>%
  summarise(mean_score = mean(score)) %>%
  ggplot(aes(x = book, y = mean_score)) + geom_bar(stat='identity')
```


## Bibliography
[@textMiningWithR]
